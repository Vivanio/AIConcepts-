{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Packages ##\n",
    "\n",
    "First, you need to import all the packages that you will need during this assignment. \n",
    "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
    "- [pandas](pandas.pydata.org/) is an important package for Python data analysis.\n",
    "- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python.\n",
    "- [jdc](https://alexhagen.github.io/jdc/) : Jupyter magic that allows defining classes over multiple jupyter notebook cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jdc\n",
    "import matplotlib.pyplot as plt\n",
    "from plotutil import plotData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Problem Statement ##\n",
    "\n",
    "    - In Section 2.1, implement the helper function sigmoid \n",
    "    - In Section 2.2, implement the helper function normalize \n",
    "            (Attention: when you call it, DON'T use self.normalize becuase it is not a part of the LogisticRegression class)\n",
    "    - In Section 2.3, define the LogisticRegression class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Sigmoid Function ###\n",
    "\n",
    "Define a helper function 1: $sigmoid(Z) = \\frac{1}{1 + e^{-Z}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of Z\n",
    "\n",
    "    Arguments:\n",
    "    Z -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(Z)\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    s = 1/(1+math.exp((0-Z)))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Feature Scaling ###\n",
    "Define helper function 2 -- features normalization:\n",
    "$ \\frac{x_{i} - mean}{\\sigma}$, where $\\sigma$ is the standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(matrix):\n",
    "    '''\n",
    "    matrix: the matrix that needs to be normalized. Note that each column represents a training example. \n",
    "         The number of columns is the the number of training examples\n",
    "    '''\n",
    "    # Calculate mean for each feature\n",
    "    # Pay attention to the value of axis = ?\n",
    "    # set keepdims=True to avoid rank-1 array\n",
    "    ### START YOUR CODE HERE ### \n",
    "    # calculate mean (1 line of code)\n",
    "    mean = np.mean(matrix)\n",
    "    # calculate standard deviation (1 line of code)\n",
    "    std = np.std(matrix)\n",
    "    # normalize the matrix based on mean and std\n",
    "    #mat = [((x-mean)/std) for x in matrix]\n",
    "    matrix = (matrix-mean)/std\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Logistic Regress Class ###\n",
    "You will create a neural network class - LogisticRegression:\n",
    "    - initialize parameters, such as weights, learning rate, etc.\n",
    "    - implement the gredient descent algorithm\n",
    "    - implement the predict function to make predictions for new data sets\n",
    "    - implement the normalization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "    def __init__(self, num_of_features=1, learning_rate=0.1):\n",
    "        \"\"\"\n",
    "        This function creates a vector of zeros of shape (num_of_features, 1) for W and initializes w_0 to 0.\n",
    "\n",
    "        Argument:\n",
    "        num_of_features -- size of the W vector, i.e., the number of features, excluding the bias\n",
    "\n",
    "        Returns:\n",
    "        W -- initialized vector of shape (num_of_features, 1)\n",
    "        w_0 -- initialized scalar (corresponds to the bias)\n",
    "        \"\"\"\n",
    "        # n is the number of features\n",
    "        self.n = num_of_features\n",
    "        # alpha is the learning rate\n",
    "        self.alpha = learning_rate\n",
    "        \n",
    "        ### START YOUR CODE HERE ### \n",
    "        #initialize self.W and self.w_0 to be 0's\n",
    "        self.W = np.zeros(shape=(self.n, 1))\n",
    "        self.w_0 = 0\n",
    "        ### YOUR CODE ENDS ###\n",
    "        assert(self.W.shape == (self.n, 1))\n",
    "        assert(isinstance(self.w_0, float) or isinstance(self.w_0, int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Gradient Descent ##\n",
    "\n",
    "Forward Propagation:\n",
    "- You get X with its shape as (n, m)\n",
    "- You compute  $$h_{W}(X) = \\hat{Y} = \\sigma(w^T X + w_{0}) = \\frac{1}{1 + e^{-(w^T x + w_{0})}}\\tag{1}$$\n",
    "- You calculate the loss function:  $$L(W) = \\frac{1}{m} \\sum_{i=1}^{m}- y^{(i)}  \\log(\\hat{y^{(i)}}) - (1-y^{(i)} )  \\log(1-\\hat{y^{(i)}})\\tag{2}$$. \n",
    "\n",
    "Here are the two formulas you will be using: \n",
    "\n",
    "$$ dw_{j} =\\frac{\\partial L}{\\partial w_{j}} = \\frac{1}{m} \\sum_{i=1}^m (( h_{W}(x^{(i)}) -y^{(i)}) * x_{j}^{(i)})\\tag{3}$$\n",
    "$$ dw_{0} = \\frac{\\partial L}{\\partial w_{0}} = \\frac{1}{m} \\sum_{i=1}^m (h_{W}(x^{(i)}) -y^{(i)})\\tag{4}$$\n",
    "\n",
    "The weights will be updated:\n",
    "$$ w_{j} = w_{j} - {\\alpha} * dw_{j}\\tag{5}$$\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to LogisticRegression\n",
    "def fit(self, X, Y, epochs=1000, print_loss=True):\n",
    "    \"\"\"\n",
    "    This function implements the Gradient Descent Algorithm\n",
    "    Arguments:\n",
    "    X -- training data matrix: each column is a training example. \n",
    "            The number of columns is equal to the number of training examples\n",
    "    Y -- true \"label\" vector: shape (1, m)\n",
    "    epochs --\n",
    "\n",
    "    Return:\n",
    "    params -- dictionary containing weights\n",
    "    losses -- loss values of every 100 epochs\n",
    "    grads -- dictionary containing dw and dw_0\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        # Get the number of training examples\n",
    "        m = X.shape[1]\n",
    "\n",
    "        ### START YOUR CODE HERE ### \n",
    "        # Calculate the hypothesis outputs A (≈ 2 lines of code)\n",
    "        Z = np.dot(self.W.t,X)+self.w_0\n",
    "        A = self.sigmoid(Z)\n",
    "        # Calculate loss (≈ 1 line of code)\n",
    "        loss = (-1/m)*(np.dot(Y,np.log(A).T) + np.dot((1-Y), np.log(1-A).T))\n",
    "        # Calculate the gredients for W and w_0\n",
    "        dw = (1/m)*(np.dot(X, (A-Y)).T)\n",
    "        dw_0 = (1/m)*(np.sum(A-Y))\n",
    "\n",
    "        # Weight updates\n",
    "        self.W = self.W-self.alpha*dw\n",
    "        self.w_0 = self.w_0-self.alpha*dw_0\n",
    "        ### YOUR CODE ENDS ###\n",
    "\n",
    "        if((i % 100) == 0):\n",
    "            losses.append(loss)\n",
    "             # Print the cost every 100 training examples\n",
    "            if print_loss:\n",
    "                print (\"Cost after iteration %i: %f\" %(i, loss))\n",
    "\n",
    "\n",
    "    params = {\n",
    "        \"W\": self.W,\n",
    "        \"w_0\": self.w_0\n",
    "    }\n",
    "\n",
    "    grads = {\n",
    "        \"dw\": dw,\n",
    "        \"dw_0\": dw_0\n",
    "    }\n",
    "\n",
    "    return params, grads, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions ###\n",
    "The predicted output is calculated as $h_{W}(X) = \\sigma(W^T * X + b)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to LogisticRegression\n",
    "def predict(self, X):\n",
    "    '''\n",
    "    Predict the actual values using learned parameters (self.W, self.w_0)\n",
    "\n",
    "    Arguments:\n",
    "    X -- data of size (n x m)\n",
    "\n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions for the examples in X\n",
    "    '''\n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "\n",
    "    # Compute the actual values\n",
    "    ### START YOUR CODE HERE ### \n",
    "    Z_prediction = self.alpha*(self.W.T*X + self.w_0)\n",
    "    A = self.sigmoid(Z_prediction)\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "\n",
    "        # Convert probabilities A[0,i] to actual predictions Y_prediction[0,i]\n",
    "        ### START CODE HERE ### (≈ 3 lines of code)\n",
    "        if A[0, i] <= 0.5:\n",
    "            Y_prediction[0, i] = 0\n",
    "        else:\n",
    "            Y_prediction[0, i] = 1\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    ### YOUR CODE ENDS ###\n",
    "    assert(Y_prediction.shape == (1, m))\n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run the Experiments ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: model\n",
    "\n",
    "def Run_Experiment(X_train, Y_train, X_test, Y_test, epochs = 2000, learning_rate = 0.5, print_loss = False):\n",
    "    \"\"\"\n",
    "    Builds the multivariate linear regression model by calling the function you've implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array \n",
    "    Y_train -- training labels represented by a numpy array (vector) \n",
    "    X_test -- test set represented by a numpy array\n",
    "    Y_test -- test labels represented by a numpy array (vector)\n",
    "    epochs -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_loss -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    num_of_features = X_train.shape[0]\n",
    "    model = LogisticRegression(num_of_features, learning_rate)\n",
    "    \n",
    "    \n",
    "    ### START YOUR CODE HERE ###\n",
    "    # Obtain the parameters, gredients, and losses by calling a model's method (≈ 1 line of code)\n",
    "    parameters, grads, losses = model.fit(X_train, Y_train, epochs, print_loss)\n",
    "    ### YOUR CODE ENDS ###\n",
    "    \n",
    "    ### START YOUR CODE HERE ###\n",
    "    # Predict test/train set examples (≈ 2 lines of code)\n",
    "    Y_prediction_test = model.predict(X_test)\n",
    "    Y_prediction_train = model.predict(X_train)\n",
    "    ### YOUR CODE ENDS ###\n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    W = parameters['W']\n",
    "    w_0 = parameters['w_0']\n",
    "    print(\"W is \" + str(W))\n",
    "    print(\"w_0 is \" + str(w_0))\n",
    "    \n",
    "    d = {\"losses\": losses,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"W\" : W, \n",
    "         \"w_0\" : w_0,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"epochs\": epochs}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data and Start the Learning Process ###\n",
    "You can change num_iterations and learning_rate to see the learning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#from plotutil import plotData\n",
    "\n",
    "data = np.loadtxt('pa3-data1-train.csv', delimiter=',')\n",
    "X_train = data[:,:-1].T\n",
    "y_train = data[:,-1].T\n",
    "\n",
    "#plot data\n",
    "plotData(X_train, y_train, \"1st exam\", \"2nd exam\")\n",
    "\n",
    "data_test = np.loadtxt('pa3-data1-test.csv', delimiter=',')\n",
    "X_test = data_test[:, :-1].T\n",
    "y_test = data_test[:, -1].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 't'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6244/550178956.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRun_Experiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.02\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# Plot learning curve (with costs)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'losses'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6244/32848783.py\u001b[0m in \u001b[0;36mRun_Experiment\u001b[1;34m(X_train, Y_train, X_test, Y_test, epochs, learning_rate, print_loss)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;31m### START YOUR CODE HERE ###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m# Obtain the parameters, gredients, and losses by calling a model's method (≈ 1 line of code)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[1;31m### YOUR CODE ENDS ###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<string>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, Y, epochs, print_loss)\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 't'"
     ]
    }
   ],
   "source": [
    "d = Run_Experiment(X_train, y_train, X_test, y_test, epochs = 2000, learning_rate = 0.02, print_loss = True)\n",
    "# Plot learning curve (with costs)\n",
    "losses = np.squeeze(d['losses'])\n",
    "plt.plot(losses)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs (per hundreds)')\n",
    "plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the learning curve ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6244/3076389720.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplotData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"1st exam\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mylabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"2nd exam\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"W\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"w_0\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'd' is not defined"
     ]
    }
   ],
   "source": [
    "plotData(normalize(X_train), y_train, xlabel=\"1st exam\", ylabel=\"2nd exam\", w = d[\"W\"], b = d[\"w_0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
